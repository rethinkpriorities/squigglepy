from functools import reduce
from hypothesis import assume, example, given, settings, Phase
import hypothesis.strategies as st
import numpy as np
import operator
from pytest import approx
from scipy import integrate, optimize, stats
import sys
from unittest.mock import patch, Mock
import warnings

from ..squigglepy.distributions import *
from ..squigglepy.numeric_distribution import numeric, NumericDistribution, _bump_indexes
from ..squigglepy import samplers, utils

# There are a lot of functions testing various combinations of behaviors with
# no obvious way to order them. These functions are ordered basically like this:
#
# 1. helper functions
# 2. tests for constructors for norm and lognorm
# 3. tests for basic operations on norm and lognorm, in the order
#    product > sum > negation > subtraction
# 4. tests for other distributions
# 5. tests for non-operation functions such as cdf/quantile
# 6. special tests, such as profiling tests
#
# Tests with `basic` in the name use hard-coded values to ensure basic
# functionality. Other tests use values generated by the hypothesis library.


def relative_error(x, y):
    if x == 0 and y == 0:
        return 0
    if x == 0:
        return abs(y)
    if y == 0:
        return abs(x)
    return max(x / y, y / x) - 1


def fix_ordering(a, b):
    """
    Check that a and b are ordered correctly and that they're not tiny enough
    to mess up floating point calculations.
    """
    if a > b:
        a, b = b, a
    assume(a != b)
    assume(((b - a) / (50 * (abs(a) + abs(b)))) ** 2 > sys.float_info.epsilon)
    assume(a == 0 or abs(a) > sys.float_info.epsilon)
    assume(b == 0 or abs(b) > sys.float_info.epsilon)
    return a, b


@given(
    mean1=st.floats(min_value=-1e5, max_value=1e5),
    mean2=st.floats(min_value=-1e5, max_value=1e5),
    sd1=st.floats(min_value=0.1, max_value=100),
    sd2=st.floats(min_value=0.001, max_value=1000),
)
@example(mean1=0, mean2=-8, sd1=1, sd2=1)
def test_sum_exact_summary_stats(mean1, mean2, sd1, sd2):
    """Test that the formulas for exact moments are implemented correctly."""
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = NormalDistribution(mean=mean2, sd=sd2)
    hist1 = numeric(dist1, warn=False)
    hist2 = numeric(dist2, warn=False)
    hist_prod = hist1 + hist2
    assert hist_prod.exact_mean == approx(
        stats.norm.mean(mean1 + mean2, np.sqrt(sd1**2 + sd2**2))
    )
    assert hist_prod.exact_sd == approx(
        stats.norm.std(
            mean1 + mean2,
            np.sqrt(sd1**2 + sd2**2),
        )
    )


@given(
    norm_mean1=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_mean2=st.floats(min_value=-np.log(1e5), max_value=np.log(1e5)),
    norm_sd1=st.floats(min_value=0.1, max_value=3),
    norm_sd2=st.floats(min_value=0.001, max_value=3),
)
def test_lognorm_product_exact_summary_stats(norm_mean1, norm_mean2, norm_sd1, norm_sd2):
    """Test that the formulas for exact moments are implemented correctly."""
    dist1 = LognormalDistribution(norm_mean=norm_mean1, norm_sd=norm_sd1)
    dist2 = LognormalDistribution(norm_mean=norm_mean2, norm_sd=norm_sd2)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        hist1 = numeric(dist1, warn=False)
        hist2 = numeric(dist2, warn=False)
    hist_prod = hist1 * hist2
    assert hist_prod.exact_mean == approx(
        stats.lognorm.mean(
            np.sqrt(norm_sd1**2 + norm_sd2**2), scale=np.exp(norm_mean1 + norm_mean2)
        )
    )
    assert hist_prod.exact_sd == approx(
        stats.lognorm.std(
            np.sqrt(norm_sd1**2 + norm_sd2**2), scale=np.exp(norm_mean1 + norm_mean2)
        )
    )


@given(
    mean=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    sd=st.floats(min_value=0.001, max_value=100),
)
def test_norm_basic(mean, sd):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, bin_sizing="uniform", warn=False)
    assert hist.est_mean() == approx(mean)
    assert hist.est_sd() == approx(sd, rel=0.01)


@given(
    norm_mean=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_sd=st.floats(min_value=0.001, max_value=3),
    bin_sizing=st.sampled_from(["uniform", "log-uniform", "ev", "mass"]),
)
@example(norm_mean=0, norm_sd=1, bin_sizing="log-uniform")
def test_lognorm_mean(norm_mean, norm_sd, bin_sizing):
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, bin_sizing=bin_sizing, warn=False)
    if bin_sizing == "ev":
        tolerance = 1e-6
    elif bin_sizing == "log-uniform":
        tolerance = 1e-2
    else:
        tolerance = 0.01 if dist.norm_sd < 3 else 0.1
    assert hist.est_mean() == approx(
        stats.lognorm.mean(dist.norm_sd, scale=np.exp(dist.norm_mean)),
        rel=tolerance,
    )


@given(
    norm_mean=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_sd=st.floats(min_value=0.01, max_value=3),
)
def test_lognorm_sd(norm_mean, norm_sd):
    test_edges = False
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, bin_sizing="log-uniform", warn=False)

    def true_variance(left, right):
        return integrate.quad(
            lambda x: (x - dist.lognorm_mean) ** 2
            * stats.lognorm.pdf(x, dist.norm_sd, scale=np.exp(dist.norm_mean)),
            left,
            right,
        )[0]

    def observed_variance(left, right):
        return np.sum(
            hist.masses[left:right] * (hist.values[left:right] - hist.est_mean()) ** 2
        )

    if test_edges:
        # Note: For bin_sizing=ev, adding more bins increases accuracy overall,
        # but decreases accuracy on the far right tail.
        midpoint = hist.values[int(hist.num_bins * 9 / 10)]
        expected_left_variance = true_variance(0, midpoint)
        expected_right_variance = true_variance(midpoint, np.inf)
        midpoint_index = int(len(hist) * hist.contribution_to_ev(midpoint))
        observed_left_variance = observed_variance(0, midpoint_index)
        observed_right_variance = observed_variance(midpoint_index, len(hist))
        print("")
        print_accuracy_ratio(observed_left_variance, expected_left_variance, "Left   ")
        print_accuracy_ratio(observed_right_variance, expected_right_variance, "Right  ")
        print_accuracy_ratio(hist.est_sd(), dist.lognorm_sd, "Overall")

    assert hist.est_sd() == approx(dist.lognorm_sd, rel=0.01 + 0.1 * norm_sd)


@given(
    mean=st.floats(min_value=-10, max_value=10),
    sd=st.floats(min_value=0.01, max_value=10),
    clip_zscore=st.floats(min_value=-4, max_value=4),
)
@example(mean=0.25, sd=6, clip_zscore=0)
def test_norm_one_sided_clip(mean, sd, clip_zscore):
    # TODO: changing allowance from 0 to 0.25 made this start failing. The
    # problem is that deleting a bit of mass on one side is shifting the EV by
    # more than the tolerance.
    tolerance = 1e-3 if abs(clip_zscore) > 3 else 1e-5
    clip = mean + clip_zscore * sd
    dist = NormalDistribution(mean=mean, sd=sd, lclip=clip)
    hist = numeric(dist, warn=False)
    assert hist.est_mean() == approx(
        stats.truncnorm.mean(clip_zscore, np.inf, loc=mean, scale=sd), rel=tolerance, abs=tolerance
    )

    # The exact mean can still be a bit off because uniform bin_sizing doesn't
    # cover the full domain
    assert hist.exact_mean == approx(
        stats.truncnorm.mean(clip_zscore, np.inf, loc=mean, scale=sd), rel=1e-5, abs=1e-9
    )

    dist = NormalDistribution(mean=mean, sd=sd, rclip=clip)
    hist = numeric(dist, warn=False)
    assert hist.est_mean() == approx(
        stats.truncnorm.mean(-np.inf, clip_zscore, loc=mean, scale=sd),
        rel=tolerance,
        abs=tolerance,
    )
    assert hist.exact_mean == approx(
        stats.truncnorm.mean(-np.inf, clip_zscore, loc=mean, scale=sd), rel=1e-5, abs=1e-6
    )


@given(
    mean=st.floats(min_value=-1, max_value=1),
    sd=st.floats(min_value=0.01, max_value=10),
    lclip_zscore=st.floats(min_value=-4, max_value=4),
    rclip_zscore=st.floats(min_value=-4, max_value=4),
)
def test_norm_clip(mean, sd, lclip_zscore, rclip_zscore):
    tolerance = 1e-3 if max(abs(lclip_zscore), abs(rclip_zscore)) > 3 else 1e-5
    if lclip_zscore > rclip_zscore:
        lclip_zscore, rclip_zscore = rclip_zscore, lclip_zscore
    assume(abs(rclip_zscore - lclip_zscore) > 0.01)
    lclip = mean + lclip_zscore * sd
    rclip = mean + rclip_zscore * sd
    dist = NormalDistribution(mean=mean, sd=sd, lclip=lclip, rclip=rclip)
    hist = numeric(dist, warn=False)

    assert hist.est_mean() == approx(
        stats.truncnorm.mean(lclip_zscore, rclip_zscore, loc=mean, scale=sd), rel=tolerance
    )
    assert hist.est_mean() == approx(hist.exact_mean, rel=tolerance)
    assert hist.exact_mean == approx(
        stats.truncnorm.mean(lclip_zscore, rclip_zscore, loc=mean, scale=sd), rel=1e-6, abs=1e-10
    )
    assert hist.exact_sd == approx(
        stats.truncnorm.std(lclip_zscore, rclip_zscore, loc=mean, scale=sd), rel=1e-6, abs=1e-10
    )


@given(
    a=st.floats(-100, -1),
    b=st.floats(1, 100),
    lclip=st.floats(-100, -1),
    rclip=st.floats(1, 100),
)
def test_uniform_clip(a, b, lclip, rclip):
    dist = UniformDistribution(a, b)
    dist.lclip = lclip
    dist.rclip = rclip
    narrow_dist = UniformDistribution(max(a, lclip), min(b, rclip))
    hist = numeric(dist)
    narrow_hist = numeric(narrow_dist)

    assert hist.est_mean() == approx(narrow_hist.exact_mean)
    assert hist.est_mean() == approx(narrow_hist.est_mean())
    assert hist.values[0] == approx(narrow_hist.values[0])
    assert hist.values[-1] == approx(narrow_hist.values[-1])


@given(
    norm_mean=st.floats(min_value=0.1, max_value=10),
    norm_sd=st.floats(min_value=0.5, max_value=3),
    clip_zscore=st.floats(min_value=-2, max_value=2),
)
def test_lognorm_clip_and_sum(norm_mean, norm_sd, clip_zscore):
    clip = np.exp(norm_mean + norm_sd * clip_zscore)
    left_dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd, rclip=clip)
    right_dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd, lclip=clip)
    left_hist = numeric(left_dist, warn=False)
    right_hist = numeric(right_dist, warn=False)
    left_mass = stats.lognorm.cdf(clip, norm_sd, scale=np.exp(norm_mean))
    right_mass = 1 - left_mass
    true_mean = stats.lognorm.mean(norm_sd, scale=np.exp(norm_mean))
    sum_exact_mean = left_mass * left_hist.exact_mean + right_mass * right_hist.exact_mean
    sum_hist_mean = (
        left_mass * left_hist.est_mean() + right_mass * right_hist.est_mean()
    )

    # TODO: the error margin is surprisingly large
    assert sum_exact_mean == approx(true_mean, rel=1e-3, abs=1e-6)
    assert sum_hist_mean == approx(true_mean, rel=1e-3, abs=1e-6)

@given(
    mean1=st.floats(min_value=-1000, max_value=0.01),
    mean2=st.floats(min_value=0.01, max_value=1000),
    mean3=st.floats(min_value=0.01, max_value=1000),
    sd1=st.floats(min_value=0.1, max_value=10),
    sd2=st.floats(min_value=0.1, max_value=10),
    sd3=st.floats(min_value=0.1, max_value=10),
    bin_sizing=st.sampled_from(["ev", "mass", "uniform"]),
)
@example(mean1=5, mean2=5, mean3=4, sd1=1, sd2=1, sd3=1, bin_sizing="ev")
@example(mean1=9, mean2=9, mean3=9, sd1=1, sd2=1, sd3=1, bin_sizing="ev")
def test_norm_product(mean1, mean2, mean3, sd1, sd2, sd3, bin_sizing):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = NormalDistribution(mean=mean2, sd=sd2)
    dist3 = NormalDistribution(mean=mean3, sd=sd3)
    mean_tolerance = 1e-5
    sd_tolerance = 0.2 if bin_sizing == "uniform" else 1

    hist1 = numeric(dist1, num_bins=40, bin_sizing=bin_sizing, warn=False)
    hist2 = numeric(dist2, num_bins=40, bin_sizing=bin_sizing, warn=False)
    hist3 = numeric(dist3, num_bins=40, bin_sizing=bin_sizing, warn=False)
    hist_prod = hist1 * hist2
    assert hist_prod.est_mean() == approx(
        dist1.mean * dist2.mean, rel=mean_tolerance, abs=1e-8
    )
    assert hist_prod.est_sd() == approx(
        np.sqrt(
            (dist1.sd**2 + dist1.mean**2) * (dist2.sd**2 + dist2.mean**2)
            - dist1.mean**2 * dist2.mean**2
        ),
        rel=sd_tolerance,
    )
    hist3_prod = hist_prod * hist3
    assert hist3_prod.est_mean() == approx(
        dist1.mean * dist2.mean * dist3.mean, rel=mean_tolerance, abs=1e-8
    )


@given(
    mean=st.floats(min_value=-10, max_value=10),
    sd=st.floats(min_value=0.001, max_value=100),
    num_bins=st.sampled_from([40, 100]),
    bin_sizing=st.sampled_from(["ev", "mass", "uniform"]),
)
@settings(max_examples=100)
def test_norm_mean_error_propagation(mean, sd, num_bins, bin_sizing):
    """ "Test how quickly the error in the mean grows as distributions are
    multiplied."""
    dist = NormalDistribution(mean=mean, sd=sd)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        hist = numeric(dist, num_bins=num_bins, bin_sizing=bin_sizing)
        hist_base = numeric(dist, num_bins=num_bins, bin_sizing=bin_sizing)
    tolerance = 1e-10 if bin_sizing == "ev" else 1e-5

    for i in range(1, 17):
        true_mean = mean**i
        true_sd = np.sqrt((dist.sd**2 + dist.mean**2) ** i - dist.mean ** (2 * i))
        if true_sd > 1e15:
            break
        assert hist.est_mean() == approx(
            true_mean, abs=tolerance ** (1 / i), rel=tolerance ** (1 / i)
        ), f"On iteration {i}"
        hist = hist * hist_base


@given(
    mean1=st.floats(min_value=-100, max_value=100),
    mean2=st.floats(min_value=-np.log(1e5), max_value=np.log(1e5)),
    mean3=st.floats(min_value=-100, max_value=100),
    sd1=st.floats(min_value=0.001, max_value=100),
    sd2=st.floats(min_value=0.001, max_value=3),
    sd3=st.floats(min_value=0.001, max_value=100),
    # num_bins1=st.sampled_from([40, 100]),
    # num_bins2=st.sampled_from([40, 100]),
    num_bins1=st.sampled_from([100]),
    num_bins2=st.sampled_from([100]),
)
@example(mean1=99, mean2=0, mean3=-1e-16, sd1=1.5, sd2=3, sd3=0.5, num_bins1=100, num_bins2=100)
def test_norm_lognorm_product_sum(mean1, mean2, mean3, sd1, sd2, sd3, num_bins1, num_bins2):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = LognormalDistribution(norm_mean=mean2, norm_sd=sd2)
    dist3 = NormalDistribution(mean=mean3, sd=sd3)
    hist1 = numeric(dist1, num_bins=num_bins1, warn=False)
    hist2 = numeric(dist2, num_bins=num_bins2, warn=False)
    hist3 = numeric(dist3, num_bins=num_bins1, warn=False)
    hist_prod = hist1 * hist2
    assert all(np.diff(hist_prod.values) >= 0)
    assert hist_prod.est_mean() == approx(hist_prod.exact_mean, abs=1e-5, rel=1e-5)

    # SD is pretty inaccurate
    sd_tolerance = 1 if num_bins1 == 100 and num_bins2 == 100 else 2
    assert hist_prod.est_sd() == approx(hist_prod.exact_sd, rel=sd_tolerance)

    hist_sum = hist_prod + hist3
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean, abs=1e-5, rel=1e-5)


@given(
    norm_mean=st.floats(min_value=np.log(1e-6), max_value=np.log(1e6)),
    norm_sd=st.floats(min_value=0.001, max_value=2),
    num_bins=st.sampled_from([40, 100]),
    bin_sizing=st.sampled_from(["ev", "log-uniform", "fat-hybrid"]),
)
@settings(max_examples=10)
def test_lognorm_mean_error_propagation(norm_mean, norm_sd, num_bins, bin_sizing):
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    hist_base = numeric(dist, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    inv_tolerance = 1 - 1e-12 if bin_sizing == "ev" else 0.98

    for i in range(1, 13):
        true_mean = stats.lognorm.mean(np.sqrt(i) * norm_sd, scale=np.exp(i * norm_mean))
        if bin_sizing == "ev":
            # log-uniform can have out-of-order values due to the masses at the
            # end being very small
            assert all(np.diff(hist.values) >= 0), f"On iteration {i}: {hist.values}"
        assert hist.est_mean() == approx(
            true_mean, rel=1 - inv_tolerance**i
        ), f"On iteration {i}"
        hist = hist * hist_base


@given(bin_sizing=st.sampled_from(["ev", "log-uniform"]))
def test_lognorm_sd_error_propagation(bin_sizing):
    verbose = False
    dist = LognormalDistribution(norm_mean=0, norm_sd=0.1)
    num_bins = 100
    hist = numeric(dist, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    abs_error = []
    rel_error = []

    if verbose:
        print("")
    for i in [1, 2, 4, 8, 16, 32]:
        oneshot = numeric(LognormalDistribution(norm_mean=0, norm_sd=0.1 * np.sqrt(i)), num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
        true_mean = stats.lognorm.mean(np.sqrt(i))
        true_sd = hist.exact_sd
        abs_error.append(abs(hist.est_sd() - true_sd))
        rel_error.append(relative_error(hist.est_sd(), true_sd))
        if verbose:
            print(f"i={i:2d}: Hist error  : {rel_error[-1] * 100:.4f}%")
            print(f"i={i:2d}: Hist / 1shot: {(rel_error[-1] / relative_error(oneshot.est_sd(), true_sd)) * 100:.0f}%")
        hist = hist * hist

    expected_error_pcts = (
        [0.9, 2.8, 9.9, 40.7, 211, 2678]
        if bin_sizing == "ev"
        else [12, 26.3, 99.8, 733, 32000, 1e9]
    )

    for i in range(len(expected_error_pcts)):
        assert rel_error[i] < expected_error_pcts[i] / 100


@given(
    norm_mean1=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_mean2=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_sd1=st.floats(min_value=0.1, max_value=3),
    norm_sd2=st.floats(min_value=0.1, max_value=3),
    bin_sizing=st.sampled_from(["ev", "log-uniform", "fat-hybrid"]),
)
@example(norm_mean1=0, norm_mean2=0, norm_sd1=1, norm_sd2=1, bin_sizing="fat-hybrid")
def test_lognorm_product(norm_mean1, norm_sd1, norm_mean2, norm_sd2, bin_sizing):
    dists = [
        LognormalDistribution(norm_mean=norm_mean2, norm_sd=norm_sd2),
        LognormalDistribution(norm_mean=norm_mean1, norm_sd=norm_sd1),
    ]
    dist_prod = LognormalDistribution(
        norm_mean=norm_mean1 + norm_mean2, norm_sd=np.sqrt(norm_sd1**2 + norm_sd2**2)
    )
    hists = [numeric(dist, bin_sizing=bin_sizing, warn=False) for dist in dists]
    hist_prod = reduce(lambda acc, hist: acc * hist, hists)

    # Lognorm width grows with e**norm_sd**2, so error tolerance grows the same way
    sd_tolerance = 1.05 ** (1 + (norm_sd1 + norm_sd2) ** 2) - 1
    mean_tolerance = 1e-3 if bin_sizing == "log-uniform" else 1e-6
    assert hist_prod.est_mean() == approx(dist_prod.lognorm_mean, rel=mean_tolerance)
    assert hist_prod.est_sd() == approx(dist_prod.lognorm_sd, rel=sd_tolerance)


@given(
    mean1=st.floats(-1e5, 1e5),
    mean2=st.floats(min_value=-1e5, max_value=1e5),
    sd1=st.floats(min_value=0.001, max_value=1e5),
    sd2=st.floats(min_value=0.001, max_value=1e5),
    num_bins=st.sampled_from([40, 100]),
    bin_sizing=st.sampled_from(["ev", "uniform"]),
)
@example(mean1=0, mean2=0, sd1=1, sd2=16, num_bins=40, bin_sizing="ev")
@example(mean1=0, mean2=0, sd1=7, sd2=1, num_bins=40, bin_sizing="ev")
def test_norm_sum(mean1, mean2, sd1, sd2, num_bins, bin_sizing):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = NormalDistribution(mean=mean2, sd=sd2)
    hist1 = numeric(dist1, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    hist2 = numeric(dist2, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    hist_sum = hist1 + hist2

    # The further apart the means are, the less accurate the SD estimate is
    distance_apart = abs(mean1 - mean2) / hist_sum.exact_sd
    sd_tolerance = 2 + 0.5 * distance_apart
    mean_tolerance = 1e-10 + 1e-10 * distance_apart

    assert all(np.diff(hist_sum.values) >= 0)
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean, abs=mean_tolerance, rel=1e-5)
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=sd_tolerance)


@given(
    norm_mean1=st.floats(min_value=-np.log(1e6), max_value=np.log(1e6)),
    norm_mean2=st.floats(min_value=-np.log(1e6), max_value=np.log(1e6)),
    norm_sd1=st.floats(min_value=0.1, max_value=3),
    norm_sd2=st.floats(min_value=0.01, max_value=3),
    bin_sizing=st.sampled_from(["ev", "log-uniform"]),
)
def test_lognorm_sum(norm_mean1, norm_mean2, norm_sd1, norm_sd2, bin_sizing):
    dist1 = LognormalDistribution(norm_mean=norm_mean1, norm_sd=norm_sd1)
    dist2 = LognormalDistribution(norm_mean=norm_mean2, norm_sd=norm_sd2)
    hist1 = numeric(dist1, bin_sizing=bin_sizing, warn=False)
    hist2 = numeric(dist2, bin_sizing=bin_sizing, warn=False)
    hist_sum = hist1 + hist2
    assert all(np.diff(hist_sum.values) >= 0), hist_sum.values
    mean_tolerance = 1e-3 if bin_sizing == "log-uniform" else 1e-6
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean, rel=mean_tolerance)

    # SD is very inaccurate because adding lognormals produces some large but
    # very low-probability values on the right tail and the only approach is to
    # either downweight them or make the histogram much wider.
    assert hist_sum.est_sd() > min(hist1.est_sd(), hist2.est_sd())
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=2)


@given(
    mean1=st.floats(min_value=-100, max_value=100),
    mean2=st.floats(min_value=-np.log(1e5), max_value=np.log(1e5)),
    sd1=st.floats(min_value=0.001, max_value=100),
    sd2=st.floats(min_value=0.001, max_value=3),
    lognorm_bin_sizing=st.sampled_from(["ev", "log-uniform", "fat-hybrid"]),
)
@example(mean1=-68, mean2=0, sd1=1, sd2=2.9, lognorm_bin_sizing="log-uniform")
def test_norm_lognorm_sum(mean1, mean2, sd1, sd2, lognorm_bin_sizing):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = LognormalDistribution(norm_mean=mean2, norm_sd=sd2)
    hist1 = numeric(dist1, warn=False)
    hist2 = numeric(dist2, bin_sizing=lognorm_bin_sizing, warn=False)
    hist_sum = hist1 + hist2
    mean_tolerance = 0.005 if lognorm_bin_sizing == "log-uniform" else 1e-6
    sd_tolerance = 0.5
    assert all(np.diff(hist_sum.values) >= 0), hist_sum.values
    assert hist_sum.est_mean() == approx(
        hist_sum.exact_mean, abs=mean_tolerance, rel=mean_tolerance
    )
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=sd_tolerance)


@given(
    norm_mean=st.floats(min_value=-10, max_value=10),
    norm_sd=st.floats(min_value=0.1, max_value=2),
)
def test_lognorm_to_const_power(norm_mean, norm_sd):
    # If you make the power bigger, mean stays pretty accurate but SD gets
    # pretty far off (>100%) for high-variance dists
    power = 1.5
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, bin_sizing="log-uniform", warn=False)

    hist_pow = hist**power
    true_dist_pow = LognormalDistribution(norm_mean=power * norm_mean, norm_sd=power * norm_sd)
    assert hist_pow.est_mean() == approx(true_dist_pow.lognorm_mean, rel=0.005)
    assert hist_pow.est_sd() == approx(true_dist_pow.lognorm_sd, rel=0.5)


@given(
    norm_mean=st.floats(min_value=-1e6, max_value=1e6),
    norm_sd=st.floats(min_value=0.001, max_value=3),
    num_bins=st.sampled_from([40, 100]),
    bin_sizing=st.sampled_from(["ev", "uniform"]),
)
def test_norm_negate(norm_mean, norm_sd, num_bins, bin_sizing):
    dist = NormalDistribution(mean=0, sd=1)
    hist = numeric(dist, warn=False)
    neg_hist = -hist
    assert neg_hist.est_mean() == approx(-hist.est_mean())
    assert neg_hist.est_sd() == approx(hist.est_sd())


@given(
    norm_mean=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_sd=st.floats(min_value=0.001, max_value=3),
    num_bins=st.sampled_from([40, 100]),
    bin_sizing=st.sampled_from(["ev", "log-uniform"]),
)
def test_lognorm_negate(norm_mean, norm_sd, num_bins, bin_sizing):
    dist = LognormalDistribution(norm_mean=0, norm_sd=1)
    hist = numeric(dist, warn=False)
    neg_hist = -hist
    assert neg_hist.est_mean() == approx(-hist.est_mean())
    assert neg_hist.est_sd() == approx(hist.est_sd())


@given(
    type_and_size=st.sampled_from(["norm-ev", "norm-uniform", "lognorm-ev"]),
    mean1=st.floats(min_value=-1e6, max_value=1e6),
    mean2=st.floats(min_value=-100, max_value=100),
    sd1=st.floats(min_value=0.001, max_value=1000),
    sd2=st.floats(min_value=0.1, max_value=5),
    num_bins=st.sampled_from([30, 100]),
)
def test_sub(type_and_size, mean1, mean2, sd1, sd2, num_bins):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2_type, bin_sizing = type_and_size.split("-")

    if dist2_type == "norm":
        dist2 = NormalDistribution(mean=mean2, sd=sd2)
        neg_dist = NormalDistribution(mean=-mean2, sd=sd2)
    elif dist2_type == "lognorm":
        dist2 = LognormalDistribution(norm_mean=mean2, norm_sd=sd2)
        # We can't negate a lognormal distribution by changing the params
        neg_dist = None

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        hist1 = numeric(dist1, num_bins=num_bins, bin_sizing=bin_sizing)
        hist2 = numeric(dist2, num_bins=num_bins, bin_sizing=bin_sizing)
    hist_diff = hist1 - hist2
    backward_diff = hist2 - hist1
    assert not any(np.isnan(hist_diff.values))
    assert all(np.diff(hist_diff.values) >= 0)
    assert hist_diff.est_mean() == approx(-backward_diff.est_mean(), rel=0.01)
    assert hist_diff.est_sd() == approx(backward_diff.est_sd(), rel=0.05)

    if neg_dist:
        neg_hist = numeric(neg_dist, num_bins=num_bins, bin_sizing=bin_sizing)
        hist_sum = hist1 + neg_hist
        assert hist_diff.est_mean() == approx(hist_sum.est_mean(), rel=0.01)
        assert hist_diff.est_sd() == approx(hist_sum.est_sd(), rel=0.05)


def test_lognorm_sub():
    dist = LognormalDistribution(norm_mean=0, norm_sd=1)
    hist = numeric(dist, warn=False)
    hist_diff = 0.97 * hist - 0.03 * hist
    assert not any(np.isnan(hist_diff.values))
    assert all(np.diff(hist_diff.values) >= 0)
    assert hist_diff.est_mean() == approx(0.94 * dist.lognorm_mean, rel=0.001)
    assert hist_diff.est_sd() == approx(hist_diff.exact_sd, rel=0.05)


@given(
    mean=st.floats(min_value=-100, max_value=100),
    sd=st.floats(min_value=0.001, max_value=1000),
    scalar=st.floats(min_value=-100, max_value=100),
)
def test_scale(mean, sd, scalar):
    assume(scalar != 0)
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, warn=False)
    scaled_hist = scalar * hist
    assert scaled_hist.est_mean() == approx(
        scalar * hist.est_mean(), abs=1e-6, rel=1e-6
    )
    assert scaled_hist.est_sd() == approx(
        abs(scalar) * hist.est_sd(), abs=1e-6, rel=1e-6
    )
    assert scaled_hist.exact_mean == approx(scalar * hist.exact_mean)
    assert scaled_hist.exact_sd == approx(abs(scalar) * hist.exact_sd)


@given(
    mean=st.floats(min_value=-100, max_value=100),
    sd=st.floats(min_value=0.001, max_value=1000),
    scalar=st.floats(min_value=-100, max_value=100),
)
def test_shift_by(mean, sd, scalar):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, warn=False)
    shifted_hist = hist + scalar
    assert shifted_hist.est_mean() == approx(
        hist.est_mean() + scalar, abs=1e-6, rel=1e-6
    )
    assert shifted_hist.est_sd() == approx(hist.est_sd(), abs=1e-6, rel=1e-6)
    assert shifted_hist.exact_mean == approx(hist.exact_mean + scalar)
    assert shifted_hist.exact_sd == approx(hist.exact_sd)
    assert shifted_hist.pos_ev_contribution - shifted_hist.neg_ev_contribution == approx(
        shifted_hist.exact_mean
    )
    if shifted_hist.zero_bin_index < len(shifted_hist.values):
        assert shifted_hist.values[shifted_hist.zero_bin_index] > 0
    if shifted_hist.zero_bin_index > 0:
        assert shifted_hist.values[shifted_hist.zero_bin_index - 1] < 0


@given(
    norm_mean=st.floats(min_value=-10, max_value=10),
    norm_sd=st.floats(min_value=0.01, max_value=2.5),
)
def test_lognorm_reciprocal(norm_mean, norm_sd):
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    reciprocal_dist = LognormalDistribution(norm_mean=-norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, bin_sizing="log-uniform", warn=False)
    reciprocal_hist = 1 / hist
    true_reciprocal_hist = numeric(reciprocal_dist, bin_sizing="log-uniform", warn=False)

    # Taking the reciprocal does lose a good bit of accuracy because bin values
    # are set as the expected value of the bin, and the EV of 1/X is pretty
    # different. Could improve accuracy by writing
    # reciprocal_contribution_to_ev functions for every distribution type, but
    # that's probably not worth it.
    assert reciprocal_hist.est_mean() == approx(reciprocal_dist.lognorm_mean, rel=0.05)
    assert reciprocal_hist.est_sd() == approx(reciprocal_dist.lognorm_sd, rel=0.2)
    assert reciprocal_hist.neg_ev_contribution == 0
    assert reciprocal_hist.pos_ev_contribution == approx(
        true_reciprocal_hist.pos_ev_contribution, rel=0.05
    )


@given(
    norm_mean1=st.floats(min_value=-10, max_value=10),
    norm_mean2=st.floats(min_value=-10, max_value=10),
    norm_sd1=st.floats(min_value=0.01, max_value=2),
    norm_sd2=st.floats(min_value=0.01, max_value=2),
    bin_sizing1=st.sampled_from(["ev", "log-uniform"]),
)
def test_lognorm_quotient(norm_mean1, norm_mean2, norm_sd1, norm_sd2, bin_sizing1):
    dist1 = LognormalDistribution(norm_mean=norm_mean1, norm_sd=norm_sd1)
    dist2 = LognormalDistribution(norm_mean=norm_mean2, norm_sd=norm_sd2)
    hist1 = numeric(dist1, bin_sizing=bin_sizing1, warn=False)
    hist2 = numeric(dist2, bin_sizing="log-uniform", warn=False)
    quotient_hist = hist1 / hist2
    true_quotient_dist = LognormalDistribution(
        norm_mean=norm_mean1 - norm_mean2, norm_sd=np.sqrt(norm_sd1**2 + norm_sd2**2)
    )
    true_quotient_hist = numeric(true_quotient_dist, bin_sizing="log-uniform", warn=False)

    assert quotient_hist.est_mean() == approx(true_quotient_hist.est_mean(), rel=0.05)
    assert quotient_hist.est_sd() == approx(true_quotient_hist.est_sd(), rel=0.2)
    assert quotient_hist.neg_ev_contribution == approx(
        true_quotient_hist.neg_ev_contribution, rel=0.01
    )
    assert quotient_hist.pos_ev_contribution == approx(
        true_quotient_hist.pos_ev_contribution, rel=0.01
    )


@given(
    mean=st.floats(min_value=-20, max_value=20),
    sd=st.floats(min_value=0.1, max_value=1),
)
@example(mean=0, sd=2)
def test_norm_exp(mean, sd):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist)
    exp_hist = hist.exp()
    true_exp_dist = LognormalDistribution(norm_mean=mean, norm_sd=sd)

    # TODO: previously with richardson, mean was accurate to 0.005, and sd to
    # 0.1, but now it's worse b/c it was using uniform before and now it's
    # using ev
    # assert exp_hist.est_mean() == approx(true_exp_dist.lognorm_mean, rel=0.005)
    # assert exp_hist.est_sd() == approx(true_exp_dist.lognorm_sd, rel=0.1)
    assert exp_hist.est_mean() == approx(true_exp_dist.lognorm_mean, rel=0.2)
    assert exp_hist.est_sd() == approx(true_exp_dist.lognorm_sd, rel=0.5)


@given(
    mean=st.floats(min_value=-20, max_value=20),
    sd=st.floats(min_value=0.1, max_value=2),
)
@settings(phases=(Phase.explicit,))
@example(mean=0, sd=1)
@example(mean=10, sd=1)
@example(mean=0, sd=2)
@example(mean=-1, sd=2)
def test_norm_exp_basic(mean, sd):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, bin_sizing="uniform")
    exp_hist = hist.exp()
    true_exp_dist = LognormalDistribution(norm_mean=mean, norm_sd=sd)
    frac = 0.9614
    print(f"({mean:2d}, {sd:d}): mean -> {relative_error(exp_hist.mean(), stats.lognorm.mean(sd, scale=np.exp(mean))) * 100:.2f}%, ppf({frac}) -> {relative_error(exp_hist.ppf(frac), stats.lognorm.ppf(frac, sd, scale=np.exp(mean))) * 100:.2f}%, sd -> {relative_error(exp_hist.est_sd(), true_exp_dist.lognorm_sd) * 100:.2f}%")


@given(
    loga=st.floats(min_value=-5, max_value=5),
    logb=st.floats(min_value=0, max_value=10),
)
@example(loga=0, logb=0.001)
@example(loga=0, logb=2)
@example(loga=-5, logb=10)
@settings(max_examples=1)
def test_uniform_exp(loga, logb):
    loga, logb = fix_ordering(loga, logb)
    dist = UniformDistribution(loga, logb)
    hist = numeric(dist)
    exp_hist = hist.exp()
    a = np.exp(loga)
    b = np.exp(logb)
    true_mean = (b - a) / np.log(b / a)
    true_sd = np.sqrt((b**2 - a**2) / (2 * np.log(b / a)) - ((b - a) / (np.log(b / a)))**2)
    assert exp_hist.est_mean() == approx(true_mean, rel=0.01)
    if not np.isnan(true_sd):
        # variance can be slightly negative due to rounding errors
        assert exp_hist.est_sd() == approx(true_sd, rel=0.2, abs=1e-5)


@given(
    mean=st.floats(min_value=-20, max_value=20),
    sd=st.floats(min_value=0.1, max_value=3),
)
def test_lognorm_log(mean, sd):
    dist = LognormalDistribution(norm_mean=mean, norm_sd=sd)
    hist = numeric(dist, warn=False)
    log_hist = hist.log()
    true_log_dist = NormalDistribution(mean=mean, sd=sd)
    true_log_hist = numeric(true_log_dist, warn=False)
    # assert log_hist.est_mean() == approx(true_log_hist.exact_mean, rel=0.005, abs=0.005)
    # assert log_hist.est_sd() == approx(true_log_hist.exact_sd, rel=0.1)
    assert log_hist.est_mean() == approx(true_log_hist.exact_mean, rel=0.2, abs=1)
    assert log_hist.est_sd() == approx(true_log_hist.exact_sd, rel=0.5)


@given(
    mean=st.floats(min_value=-20, max_value=20),
    sd=st.floats(min_value=0.1, max_value=10),
)
@settings(max_examples=10)
def test_norm_abs(mean, sd):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = abs(numeric(dist))
    shape = abs(mean) / sd
    scale = sd
    true_mean = stats.foldnorm.mean(shape, loc=0, scale=scale)
    true_sd = stats.foldnorm.std(shape, loc=0, scale=scale)
    assert hist.est_mean() == approx(true_mean, rel=0.001)
    assert hist.est_sd() == approx(true_sd, rel=0.01)


@given(
    mean=st.floats(min_value=-20, max_value=20),
    sd=st.floats(min_value=0.1, max_value=10),
    left_zscore=st.floats(min_value=-2, max_value=2),
    zscore_width=st.floats(min_value=2, max_value=5),
)
@settings(max_examples=10)
def test_given_value_satisfies(mean, sd, left_zscore, zscore_width):
    right_zscore = left_zscore + zscore_width
    left = mean + left_zscore * sd
    right = mean + right_zscore * sd
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist).given_value_satisfies(lambda x: x >= left and x <= right)
    true_mean = stats.truncnorm.mean(left_zscore, right_zscore, loc=mean, scale=sd)
    true_sd = stats.truncnorm.std(left_zscore, right_zscore, loc=mean, scale=sd)
    assert hist.est_mean() == approx(true_mean, rel=0.1, abs=0.05)
    assert hist.est_sd() == approx(true_sd, rel=0.1, abs=0.05)


def test_probability_value_satisfies():
    dist = NormalDistribution(mean=0, sd=1)
    hist = numeric(dist)
    prob1 = hist.probability_value_satisfies(lambda x: x >= 0)
    assert prob1 == approx(0.5, rel=0.01)
    prob2 = hist.probability_value_satisfies(lambda x: x < 1)
    assert prob2 == approx(stats.norm.cdf(1), rel=0.01)


@given(
    a=st.floats(min_value=1e-6, max_value=1),
    b=st.floats(min_value=1e-6, max_value=1),
)
@example(a=1, b=1e-5)
def test_mixture(a, b):
    if a + b > 1:
        scale = a + b
        a /= scale
        b /= scale
    c = max(0, 1 - a - b)  # do max to fix floating point rounding
    dist1 = NormalDistribution(mean=0, sd=5)
    dist2 = NormalDistribution(mean=5, sd=3)
    dist3 = NormalDistribution(mean=-1, sd=1)
    mixture = MixtureDistribution([dist1, dist2, dist3], [a, b, c])
    hist = numeric(mixture, bin_sizing="uniform")
    assert hist.est_mean() == approx(
        a * dist1.mean + b * dist2.mean + c * dist3.mean, rel=1e-4
    )
    assert hist.values[0] < 0


def test_disjoint_mixture():
    dist = LognormalDistribution(norm_mean=0, norm_sd=1)
    hist1 = numeric(dist)
    hist2 = -numeric(dist)
    mixture = NumericDistribution.mixture([hist1, hist2], [0.97, 0.03], warn=False)
    assert mixture.est_mean() == approx(0.94 * dist.lognorm_mean, rel=0.001)
    assert mixture.values[0] < 0
    assert mixture.values[1] < 0
    assert mixture.values[-1] > 0
    assert mixture.contribution_to_ev(0) == approx(0.03, rel=0.1)


def test_mixture_distributivity():
    one_sided_dist = LognormalDistribution(norm_mean=0, norm_sd=1)
    ratio = [0.03, 0.97]
    product_of_mixture = numeric(mixture([-one_sided_dist, one_sided_dist], ratio)) * numeric(one_sided_dist)
    mixture_of_products = numeric(mixture([-one_sided_dist * one_sided_dist, one_sided_dist * one_sided_dist], ratio))

    assert product_of_mixture.exact_mean == approx(mixture_of_products.exact_mean, rel=1e-5)
    assert product_of_mixture.exact_sd == approx(mixture_of_products.exact_sd, rel=1e-5)
    assert product_of_mixture.est_mean() == approx(mixture_of_products.est_mean(), rel=1e-5)
    assert product_of_mixture.est_sd() == approx(mixture_of_products.est_sd(), rel=1e-2)
    assert product_of_mixture.ppf(0.5) == approx(mixture_of_products.ppf(0.5), rel=1e-3)


@given(lclip=st.integers(-4, 4), width=st.integers(1, 4))
@example(lclip=4, width=1)
def test_numeric_clip(lclip, width):
    rclip = lclip + width
    dist = NormalDistribution(mean=0, sd=1)
    full_hist = numeric(dist, num_bins=200, bin_sizing='uniform', warn=False)
    clipped_hist = full_hist.clip(lclip, rclip)
    assert clipped_hist.est_mean() == approx(stats.truncnorm.mean(lclip, rclip), rel=0.1)
    hist_sum = clipped_hist + full_hist
    assert hist_sum.est_mean() == approx(
        stats.truncnorm.mean(lclip, rclip) + stats.norm.mean(), rel=0.1
    )


@given(
    a=st.sampled_from([0.2, 0.3, 0.5, 0.7, 0.8]),
    lclip=st.sampled_from([-1, 1, None]),
    clip_width=st.sampled_from([2, 3, None]),
    bin_sizing=st.sampled_from(["uniform", "ev", "mass"]),
    # Only clip inner or outer dist b/c clipping both makes it hard to
    # calculate what the mean should be
    clip_inner=st.booleans(),
)
@example(a=0.3, lclip=-1, clip_width=2, bin_sizing="ev", clip_inner=False)
def test_sum2_clipped(a, lclip, clip_width, bin_sizing, clip_inner):
    # Clipped NumericDist accuracy really benefits from more bins. It's not
    # very accurate with 100 bins because a clipped histogram might end up with
    # only 10 bins or so.
    num_bins = 500 if not clip_inner and bin_sizing == "uniform" else 100
    clip_outer = not clip_inner
    b = max(0, 1 - a)  # do max to fix floating point rounding
    rclip = lclip + clip_width if lclip is not None and clip_width is not None else np.inf
    if lclip is None:
        lclip = -np.inf
    dist1 = NormalDistribution(
        mean=0,
        sd=1,
        lclip=lclip if clip_inner else None,
        rclip=rclip if clip_inner else None,
    )
    dist2 = NormalDistribution(mean=1, sd=2)
    hist = a * numeric(dist1, num_bins, bin_sizing, warn=False) + b * numeric(
        dist2, num_bins, bin_sizing, warn=False
    )
    if clip_outer:
        hist = hist.clip(lclip, rclip)
    if clip_inner:
        # Truncating then adding is more accurate than adding then truncating,
        # which is good because truncate-then-add is the more typical use case
        true_mean = a * stats.truncnorm.mean(lclip, rclip, 0, 1) + b * dist2.mean
        tolerance = 0.01
    else:
        mixed_mean = a * dist1.mean + b * dist2.mean
        mixed_sd = np.sqrt(a**2 * dist1.sd**2 + b**2 * dist2.sd**2)
        lclip_zscore = (lclip - mixed_mean) / mixed_sd
        rclip_zscore = (rclip - mixed_mean) / mixed_sd

        true_mean = stats.truncnorm.mean(
            lclip_zscore,
            rclip_zscore,
            mixed_mean,
            mixed_sd,
        )
        tolerance = 0.25

    assert hist.est_mean() == approx(true_mean, rel=tolerance)


@given(
    a=st.floats(min_value=1e-6, max_value=1),
    b=st.floats(min_value=1e-6, max_value=1),
    lclip=st.sampled_from([-1, 1, None]),
    clip_width=st.sampled_from([1, 3, None]),
    bin_sizing=st.sampled_from(["uniform", "ev", "mass"]),
    # Only clip inner or outer dist b/c clipping both makes it hard to
    # calculate what the mean should be
    clip_inner=st.booleans(),
)
def test_sum3_clipped(a, b, lclip, clip_width, bin_sizing, clip_inner):
    # Clipped sum accuracy really benefits from more bins. It's not very
    # accurate with 100 bins
    num_bins = 500 if not clip_inner else 100
    clip_outer = not clip_inner
    if a + b > 1:
        scale = a + b
        a /= scale
        b /= scale
    c = max(0, 1 - a - b)  # do max to fix floating point rounding
    rclip = lclip + clip_width if lclip is not None and clip_width is not None else np.inf
    if lclip is None:
        lclip = -np.inf
    dist1 = NormalDistribution(
        mean=0,
        sd=1,
        lclip=lclip if clip_inner else None,
        rclip=rclip if clip_inner else None,
    )
    dist2 = NormalDistribution(mean=1, sd=2)
    dist3 = NormalDistribution(mean=-1, sd=0.75)
    dist_sum = a * dist1 + b * dist2 + c * dist3
    if clip_outer:
        dist_sum.lclip = lclip
        dist_sum.rclip = rclip

    hist = numeric(dist_sum, num_bins=num_bins, bin_sizing=bin_sizing, warn=False)
    if clip_inner:
        true_mean = a * stats.truncnorm.mean(lclip, rclip, 0, 1) + b * dist2.mean + c * dist3.mean
        tolerance = 0.01
    else:
        mixed_mean = a * dist1.mean + b * dist2.mean + c * dist3.mean
        mixed_sd = np.sqrt(
            a**2 * dist1.sd**2 + b**2 * dist2.sd**2 + c**2 * dist3.sd**2
        )
        lclip_zscore = (lclip - mixed_mean) / mixed_sd
        rclip_zscore = (rclip - mixed_mean) / mixed_sd
        true_mean = stats.truncnorm.mean(
            lclip_zscore,
            rclip_zscore,
            mixed_mean,
            mixed_sd,
        )
        tolerance = 0.1
    assert hist.est_mean() == approx(true_mean, rel=tolerance, abs=tolerance / 10)


def test_sum_with_zeros():
    dist1 = NormalDistribution(mean=3, sd=1)
    dist2 = NormalDistribution(mean=2, sd=1)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist2 = hist2.condition_on_success(0.75)
    assert hist2.exact_mean == approx(1.5)
    assert hist2.est_mean() == approx(1.5, rel=1e-5)
    assert hist2.est_sd() == approx(hist2.exact_sd, rel=1e-3)
    hist_sum = hist1 + hist2
    assert hist_sum.exact_mean == approx(4.5)
    assert hist_sum.est_mean() == approx(4.5, rel=1e-5)


def test_product_with_zeros():
    dist1 = LognormalDistribution(norm_mean=1, norm_sd=1)
    dist2 = LognormalDistribution(norm_mean=2, norm_sd=1)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist1 = hist1.condition_on_success(2 / 3)
    hist2 = hist2.condition_on_success(0.5)
    assert hist2.exact_mean == approx(dist2.lognorm_mean / 2)
    assert hist2.est_mean() == approx(dist2.lognorm_mean / 2, rel=1e-5)
    hist_prod = hist1 * hist2
    dist_prod = LognormalDistribution(norm_mean=3, norm_sd=np.sqrt(2))
    assert hist_prod.exact_mean == approx(dist_prod.lognorm_mean / 3)
    assert hist_prod.est_mean() == approx(dist_prod.lognorm_mean / 3, rel=1e-5)


def test_shift_with_zeros():
    dist = NormalDistribution(mean=1, sd=1)
    wrapped_hist = numeric(dist, warn=False)
    hist = wrapped_hist.condition_on_success(0.5)
    shifted_hist = hist + 2
    assert shifted_hist.exact_mean == approx(2.5)
    assert shifted_hist.est_mean() == approx(2.5, rel=1e-5)
    assert shifted_hist.masses[np.searchsorted(shifted_hist.values, 2)] == approx(0.5)
    assert shifted_hist.est_sd() == approx(hist.est_sd(), rel=1e-3)
    assert shifted_hist.est_sd() == approx(shifted_hist.exact_sd, rel=1e-3)


def test_abs_with_zeros():
    dist = NormalDistribution(mean=1, sd=2)
    wrapped_hist = numeric(dist, warn=False)
    hist = wrapped_hist.condition_on_success(0.5)
    abs_hist = abs(hist)
    true_mean = stats.foldnorm.mean(1 / 2, loc=0, scale=2)
    assert abs_hist.est_mean() == approx(0.5 * true_mean, rel=0.001)


def test_condition_on_success():
    dist1 = NormalDistribution(mean=4, sd=2)
    dist2 = LognormalDistribution(norm_mean=-1, norm_sd=1)
    hist = numeric(dist1)
    event = numeric(dist2)
    outcome = hist.condition_on_success(event)
    assert outcome.exact_mean == approx(hist.exact_mean * dist2.lognorm_mean)


def test_probability_value_satisfies_with_zeros():
    dist = NormalDistribution(mean=0, sd=1)
    hist = numeric(dist).condition_on_success(0.5)
    assert hist.probability_value_satisfies(lambda x: x == 0) == approx(0.5)
    assert hist.probability_value_satisfies(lambda x: x != 0) == approx(0.5)
    assert hist.probability_value_satisfies(lambda x: x > 0) == approx(0.25)
    assert hist.probability_value_satisfies(lambda x: x >= 0) == approx(0.75)


def test_quantile_with_zeros():
    mean = 1
    sd = 1
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, bin_sizing="uniform", warn=False).condition_on_success(0.25)

    tolerance = 0.01

    # When we scale down by 4x, the quantile that used to be at q is now at 4q
    assert hist.quantile(0.025) == approx(stats.norm.ppf(0.1, mean, sd), rel=tolerance)
    assert hist.quantile(stats.norm.cdf(-0.01, mean, sd) / 4) == approx(
        -0.01, rel=tolerance, abs=1e-3
    )

    # The values in the ~middle 75% equal 0
    assert hist.quantile(stats.norm.cdf(0.01, mean, sd) / 4) == 0
    assert hist.quantile(0.4 + stats.norm.cdf(0.01, mean, sd) / 4) == 0

    # The values above 0 work like the values below 0
    assert hist.quantile(0.75 + stats.norm.cdf(0.01, mean, sd) / 4) == approx(
        0.01, rel=tolerance, abs=1e-3
    )
    assert hist.quantile([0.99]) == approx([stats.norm.ppf(0.96, mean, sd)], rel=tolerance)


@given(
    a=st.floats(min_value=-100, max_value=100),
    b=st.floats(min_value=-100, max_value=100),
)
@settings(max_examples=10)
def test_uniform_basic(a, b):
    a, b = fix_ordering(a, b)
    dist = UniformDistribution(x=a, y=b)
    with warnings.catch_warnings():
        # hypothesis generates some extremely tiny input params, which
        # generates warnings about EV contributions being 0.
        warnings.simplefilter("ignore")
        hist = numeric(dist)
    assert hist.est_mean() == approx((a + b) / 2, 1e-6)
    assert hist.est_sd() == approx(np.sqrt(1 / 12 * (b - a) ** 2), rel=1e-3)


def test_uniform_sum_basic():
    # The sum of standard uniform distributions is also known as an Irwin-Hall
    # distribution:
    # https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution
    dist = UniformDistribution(0, 1)
    hist1 = numeric(dist)
    hist_sum = numeric(dist)
    hist_sum += hist1
    assert hist_sum.exact_mean == approx(1)
    assert hist_sum.exact_sd == approx(np.sqrt(2 / 12))
    assert hist_sum.est_mean() == approx(1)
    assert hist_sum.est_sd() == approx(np.sqrt(2 / 12), rel=0.005)
    hist_sum += hist1
    assert hist_sum.est_mean() == approx(1.5)
    assert hist_sum.est_sd() == approx(np.sqrt(3 / 12), rel=0.005)
    hist_sum += hist1
    assert hist_sum.est_mean() == approx(2)
    assert hist_sum.est_sd() == approx(np.sqrt(4 / 12), rel=0.005)


@given(
    # I originally had both dists on [-1000, 1000] but then hypothesis would
    # generate ~90% of cases with extremely tiny values that are too small for
    # floating point operations to handle, so I forced most of the values to be
    # at least a little away from 0.
    a1=st.floats(min_value=-1000, max_value=0.001),
    b1=st.floats(min_value=0.001, max_value=1000),
    a2=st.floats(min_value=0, max_value=1000),
    b2=st.floats(min_value=1, max_value=10000),
    flip2=st.booleans(),
)
def test_uniform_sum(a1, b1, a2, b2, flip2):
    if flip2:
        a2, b2 = -b2, -a2
    a1, b1 = fix_ordering(a1, b1)
    a2, b2 = fix_ordering(a2, b2)
    dist1 = UniformDistribution(x=a1, y=b1)
    dist2 = UniformDistribution(x=a2, y=b2)
    with warnings.catch_warnings():
        # hypothesis generates some extremely tiny input params, which
        # generates warnings about EV contributions being 0.
        warnings.simplefilter("ignore")
        hist1 = numeric(dist1)
        hist2 = numeric(dist2)

    hist_sum = hist1 + hist2
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean)
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=0.01)


@given(
    a1=st.floats(min_value=-1000, max_value=0.001),
    b1=st.floats(min_value=0.001, max_value=1000),
    a2=st.floats(min_value=0, max_value=1000),
    b2=st.floats(min_value=1, max_value=10000),
    flip2=st.booleans(),
)
@settings(max_examples=10)
def test_uniform_prod(a1, b1, a2, b2, flip2):
    if flip2:
        a2, b2 = -b2, -a2
    a1, b1 = fix_ordering(a1, b1)
    a2, b2 = fix_ordering(a2, b2)
    dist1 = UniformDistribution(x=a1, y=b1)
    dist2 = UniformDistribution(x=a2, y=b2)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        hist1 = numeric(dist1)
        hist2 = numeric(dist2)
    hist_prod = hist1 * hist2
    assert hist_prod.est_mean() == approx(hist_prod.exact_mean, abs=1e-6, rel=1e-6)
    assert hist_prod.est_sd() == approx(hist_prod.exact_sd, rel=0.01)


@given(
    a=st.floats(min_value=-1000, max_value=0.001),
    b=st.floats(min_value=0.001, max_value=1000),
    norm_mean=st.floats(np.log(0.001), np.log(1e6)),
    norm_sd=st.floats(0.1, 2),
)
@settings(max_examples=10)
def test_uniform_lognorm_prod(a, b, norm_mean, norm_sd):
    a, b = fix_ordering(a, b)
    dist1 = UniformDistribution(x=a, y=b)
    dist2 = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2, warn=False)
    hist_prod = hist1 * hist2
    assert hist_prod.est_mean() == approx(hist_prod.exact_mean, rel=1e-7, abs=1e-7)
    assert hist_prod.est_sd() == approx(hist_prod.exact_sd, rel=0.1)


@given(
    a=st.floats(min_value=0.5, max_value=100),
    b=st.floats(min_value=0.5, max_value=100),
)
@settings(max_examples=10)
def test_beta_basic(a, b):
    dist = BetaDistribution(a, b)
    hist = numeric(dist)
    assert hist.exact_mean == approx(a / (a + b))
    assert hist.exact_sd == approx(np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1))))
    assert hist.est_mean() == approx(hist.exact_mean)
    assert hist.est_sd() == approx(hist.exact_sd, rel=0.02)


@given(
    a=st.floats(min_value=1, max_value=100),
    b=st.floats(min_value=1, max_value=100),
    mean=st.floats(-10, 10),
    sd=st.floats(0.1, 10),
)
@settings(max_examples=10)
def test_beta_sum(a, b, mean, sd):
    dist1 = BetaDistribution(a=a, b=b)
    dist2 = NormalDistribution(mean=mean, sd=sd)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist_sum = hist1 + hist2
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean, rel=1e-7, abs=1e-7)
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=0.01)


@given(
    a=st.floats(min_value=1, max_value=100),
    b=st.floats(min_value=1, max_value=100),
    norm_mean=st.floats(-10, 10),
    norm_sd=st.floats(0.1, 1),
)
@settings(max_examples=10)
def test_beta_prod(a, b, norm_mean, norm_sd):
    dist1 = BetaDistribution(a=a, b=b)
    dist2 = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist_prod = hist1 * hist2
    assert hist_prod.est_mean() == approx(hist_prod.exact_mean, rel=1e-7, abs=1e-7)
    assert hist_prod.est_sd() == approx(hist_prod.exact_sd, rel=0.02)


@given(
    left=st.floats(min_value=-100, max_value=100),
    right=st.floats(min_value=-100, max_value=100),
    mode=st.floats(min_value=-100, max_value=100),
)
@settings(max_examples=10)
def test_pert_basic(left, right, mode):
    left, mode = fix_ordering(left, mode)
    mode, right = fix_ordering(mode, right)
    left, mode = fix_ordering(left, mode)
    assert (left < mode < right) or (left == mode == right)
    dist = PERTDistribution(left=left, right=right, mode=mode)
    hist = numeric(dist)
    assert hist.exact_mean == approx((left + 4 * mode + right) / 6)
    assert hist.est_mean() == approx(hist.exact_mean)
    assert hist.exact_sd == approx((np.sqrt((hist.exact_mean - left) * (right - hist.exact_mean) / 7)))
    assert hist.est_sd() == approx(hist.exact_sd, rel=0.001)


@given(
    left=st.floats(min_value=-100, max_value=100),
    right=st.floats(min_value=-100, max_value=100),
    mode=st.floats(min_value=-100, max_value=100),
    lam=st.floats(min_value=1, max_value=10),
)
@settings(max_examples=10)
def test_pert_with_lambda(left, right, mode, lam):
    left, mode = fix_ordering(left, mode)
    mode, right = fix_ordering(mode, right)
    left, mode = fix_ordering(left, mode)
    assert (left < mode < right) or (left == mode == right)
    dist = PERTDistribution(left=left, right=right, mode=mode, lam=lam)
    hist = numeric(dist)
    true_mean = (left + lam * mode + right) / (lam + 2)
    true_sd_lam4 = np.sqrt((hist.exact_mean - left) * (right - hist.exact_mean) / 7)
    assert hist.exact_mean == approx(true_mean)
    assert hist.est_mean() == approx(true_mean)
    if lam < 3.9:
        assert hist.est_sd() > true_sd_lam4
    elif lam > 4.1:
        assert hist.est_sd() < true_sd_lam4


@given(
    mode=st.floats(min_value=0.01, max_value=0.99),
)
@settings(max_examples=10)
def test_pert_equals_beta(mode):
    alpha = 1 + 4 * mode
    beta = 1 + 4 * (1 - mode)
    beta_dist = BetaDistribution(alpha, beta)
    pert_dist = PERTDistribution(left=0, right=1, mode=mode)
    beta_hist = numeric(beta_dist)
    pert_hist = numeric(pert_dist)
    assert beta_hist.exact_mean == approx(pert_hist.exact_mean)
    assert beta_hist.exact_sd == approx(pert_hist.exact_sd)
    assert beta_hist.est_mean() == approx(pert_hist.est_mean())
    assert beta_hist.est_sd() == approx(pert_hist.est_sd(), rel=0.01)


@given(
    shape=st.floats(min_value=0.1, max_value=100),
    scale=st.floats(min_value=0.1, max_value=100),
    bin_sizing=st.sampled_from(["uniform", "ev", "mass", "fat-hybrid"]),
)
def test_gamma_basic(shape, scale, bin_sizing):
    dist = GammaDistribution(shape=shape, scale=scale)
    hist = numeric(dist, bin_sizing=bin_sizing, warn=False)
    assert hist.exact_mean == approx(shape * scale)
    assert hist.exact_sd == approx(np.sqrt(shape) * scale)
    assert hist.est_mean() == approx(hist.exact_mean)
    assert hist.est_sd() == approx(hist.exact_sd, rel=0.1)


@given(
    shape=st.floats(min_value=0.1, max_value=100),
    scale=st.floats(min_value=0.1, max_value=100),
    mean=st.floats(-10, 10),
    sd=st.floats(0.1, 10),
)
def test_gamma_sum(shape, scale, mean, sd):
    dist1 = GammaDistribution(shape=shape, scale=scale)
    dist2 = NormalDistribution(mean=mean, sd=sd)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist_sum = hist1 + hist2
    assert hist_sum.est_mean() == approx(hist_sum.exact_mean, rel=1e-7, abs=1e-7)
    assert hist_sum.est_sd() == approx(hist_sum.exact_sd, rel=0.01)


@given(
    shape=st.floats(min_value=0.1, max_value=100),
    scale=st.floats(min_value=0.1, max_value=100),
    mean=st.floats(-10, 10),
    sd=st.floats(0.1, 10),
)
def test_gamma_product(shape, scale, mean, sd):
    dist1 = GammaDistribution(shape=shape, scale=scale)
    dist2 = NormalDistribution(mean=mean, sd=sd)
    hist1 = numeric(dist1)
    hist2 = numeric(dist2)
    hist_prod = hist1 * hist2
    assert hist_prod.est_mean() == approx(hist_prod.exact_mean, rel=1e-7, abs=1e-7)
    assert hist_prod.est_sd() == approx(hist_prod.exact_sd, rel=0.01)


@given(
    df=st.floats(min_value=0.1, max_value=100),
)
def test_chi_square(df):
    dist = ChiSquareDistribution(df=df)
    hist = numeric(dist)
    assert hist.exact_mean == approx(df)
    assert hist.exact_sd == approx(np.sqrt(2 * df))
    assert hist.est_mean() == approx(hist.exact_mean)
    assert hist.est_sd() == approx(hist.exact_sd, rel=0.01)


@given(
    scale=st.floats(min_value=0.1, max_value=1e6),
)
def test_exponential_dist(scale):
    dist = ExponentialDistribution(scale=scale)
    hist = numeric(dist)
    assert hist.exact_mean == approx(scale)
    assert hist.exact_sd == approx(scale)
    assert hist.est_mean() == approx(hist.exact_mean)
    assert hist.est_sd() == approx(hist.exact_sd, rel=0.01)


@given(
    shape=st.floats(min_value=1.1, max_value=100),
)
def test_pareto_dist(shape):
    dist = ParetoDistribution(shape)
    hist = numeric(dist, warn=shape >= 2)
    assert hist.exact_mean == approx(shape / (shape - 1))
    assert hist.est_mean() == approx(hist.exact_mean, rel=0.01 / (shape - 1))
    if shape <= 2:
        assert hist.exact_sd == approx(np.inf)
    else:
        assert hist.est_sd() == approx(
            hist.exact_sd, rel=max(0.01, 0.1 / (shape - 2))
        )


@given(
    x=st.floats(min_value=-100, max_value=100),
    wrap_in_dist=st.booleans(),
)
@example(x=1, wrap_in_dist=False)
def test_constant_dist(x, wrap_in_dist):
    dist1 = NormalDistribution(mean=1, sd=1)
    if wrap_in_dist:
        dist2 = ConstantDistribution(x=x)
    else:
        dist2 = x
    hist1 = numeric(dist1, warn=False)
    hist2 = numeric(dist2, warn=False)
    hist_sum = hist1 + hist2
    assert hist_sum.exact_mean == approx(1 + x)
    assert hist_sum.est_mean() == approx(1 + x, rel=1e-6)
    assert hist_sum.exact_sd == approx(1)
    assert hist_sum.est_sd() == approx(hist1.est_sd(), rel=1e-3)


@given(
    p=st.floats(min_value=0.001, max_value=0.999),
)
def test_bernoulli_dist(p):
    dist = BernoulliDistribution(p=p)
    hist = numeric(dist, warn=False)
    assert hist.exact_mean == approx(p)
    assert hist.est_mean() == approx(p, rel=1e-6)
    assert hist.exact_sd == approx(np.sqrt(p * (1 - p)))
    assert hist.est_sd() == approx(hist.exact_sd, rel=1e-6)


def test_complex_dist():
    left = NormalDistribution(mean=1, sd=1)
    right = NormalDistribution(mean=0, sd=1)
    dist = ComplexDistribution(left, right, operator.add)
    hist = numeric(dist, warn=False)
    assert hist.exact_mean == approx(1)
    assert hist.est_mean() == approx(1, rel=1e-6)


def test_complex_dist_with_float():
    left = NormalDistribution(mean=1, sd=1)
    right = 2
    dist = ComplexDistribution(left, right, operator.mul)
    hist = numeric(dist, warn=False)
    assert hist.exact_mean == approx(2)
    assert hist.est_mean() == approx(2, rel=1e-6)


@given(
    mean=st.floats(min_value=-10, max_value=10),
    sd=st.floats(min_value=0.01, max_value=10),
    bin_num=st.integers(min_value=5, max_value=95),
)
def test_numeric_dist_contribution_to_ev(mean, sd, bin_num):
    fraction = bin_num / 100
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, bin_sizing="uniform", num_bins=100, warn=False)
    assert hist.contribution_to_ev(dist.inv_contribution_to_ev(fraction)) == approx(fraction, rel=0.01)


@given(
    norm_mean=st.floats(min_value=-np.log(1e9), max_value=np.log(1e9)),
    norm_sd=st.floats(min_value=0.001, max_value=4),
    bin_num=st.integers(min_value=2, max_value=98),
)
def test_numeric_dist_inv_contribution_to_ev(norm_mean, norm_sd, bin_num):
    # The nth value stored in the PMH represents a value between the nth and n+1th edges
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, bin_sizing="ev", warn=False)
    fraction = bin_num / hist.num_bins
    prev_fraction = fraction - 1 / hist.num_bins
    next_fraction = fraction
    assert hist.inv_contribution_to_ev(fraction) > dist.inv_contribution_to_ev(prev_fraction)
    assert hist.inv_contribution_to_ev(fraction) < dist.inv_contribution_to_ev(next_fraction)


@given(
    mean=st.floats(min_value=-100, max_value=100),
    sd=st.floats(min_value=0.01, max_value=100),
    percent=st.integers(min_value=0, max_value=100),
)
@example(mean=0, sd=1, percent=100)
def test_quantile_uniform(mean, sd, percent):
    # Note: Quantile interpolation can sometimes give incorrect results at the
    # 0th percentile because if the first two bin edges are extremely close to
    # 0, the values can be out of order due to floating point rounding.
    assume(percent != 0 or abs(mean) / sd < 3)
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, num_bins=200, bin_sizing="uniform", warn=False)
    if percent == 0:
        assert hist.percentile(percent) <= hist.values[0]
    elif percent == 100:
        cum_mass = np.cumsum(hist.masses) - 0.5 * hist.masses
        nonzero_indexes = [i for (i, d) in enumerate(np.diff(cum_mass)) if d > 0]
        last_valid_index = max(nonzero_indexes)
        assert hist.percentile(percent) >= hist.values[last_valid_index]
    else:
        assert hist.percentile(percent) == approx(
            stats.norm.ppf(percent / 100, loc=mean, scale=sd), rel=0.02, abs=0.02
        )


@given(
    norm_mean=st.floats(min_value=-5, max_value=5),
    norm_sd=st.floats(min_value=0.1, max_value=2),
    percent=st.integers(min_value=1, max_value=100),
)
def test_quantile_log_uniform(norm_mean, norm_sd, percent):
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, num_bins=200, bin_sizing="log-uniform", warn=False)
    if percent == 0:
        assert hist.percentile(percent) <= hist.values[0]
    elif percent == 100:
        cum_mass = np.cumsum(hist.masses) - 0.5 * hist.masses
        nonzero_indexes = [i for (i, d) in enumerate(np.diff(cum_mass)) if d > 0]
        last_valid_index = max(nonzero_indexes)
        assert hist.percentile(percent) >= hist.values[last_valid_index]
    else:
        assert hist.percentile(percent) == approx(
            stats.lognorm.ppf(percent / 100, norm_sd, scale=np.exp(norm_mean)), rel=0.01
        )


@given(
    norm_mean=st.floats(min_value=-5, max_value=5),
    norm_sd=st.floats(min_value=0.1, max_value=2),
    # Don't try smaller percentiles because the smaller bins have a lot of
    # probability mass
    percent=st.integers(min_value=20, max_value=99),
)
def test_quantile_ev(norm_mean, norm_sd, percent):
    dist = LognormalDistribution(norm_mean=norm_mean, norm_sd=norm_sd)
    hist = numeric(dist, num_bins=200, bin_sizing="ev", warn=False)
    tolerance = 0.1 if percent < 70 else 0.01
    assert hist.percentile(percent) == approx(
        stats.lognorm.ppf(percent / 100, norm_sd, scale=np.exp(norm_mean)), rel=tolerance
    )


@given(
    mean=st.floats(min_value=100, max_value=100),
    sd=st.floats(min_value=0.01, max_value=100),
    percent=st.floats(min_value=1, max_value=99),
)
@example(mean=0, sd=1, percent=1)
def test_quantile_mass(mean, sd, percent):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, num_bins=200, bin_sizing="mass", warn=False)

    # It's hard to make guarantees about how close the value will be, but we
    # should know for sure that the cdf of the value is very close to the
    # percent. Naive interpolation should have a maximum absolute error of 1 /
    # num_bins.
    assert 100 * stats.norm.cdf(hist.percentile(percent), mean, sd) == approx(percent, abs=0.1)


@given(
    mean=st.floats(min_value=100, max_value=100),
    sd=st.floats(min_value=0.01, max_value=100),
)
def test_cdf_mass(mean, sd):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, num_bins=200, bin_sizing="mass", warn=False)

    # should definitely be accurate to within 1 / num_bins but a smart interpolator
    # can do better
    tolerance = 0.001
    assert hist.cdf(mean) == approx(0.5, abs=tolerance)
    assert hist.cdf(mean - sd) == approx(stats.norm.cdf(-1), abs=tolerance)
    assert hist.cdf(mean + 2 * sd) == approx(stats.norm.cdf(2), abs=tolerance)


@given(
    mean=st.floats(min_value=100, max_value=100),
    sd=st.floats(min_value=0.01, max_value=100),
    percent=st.integers(min_value=1, max_value=99),
)
def test_cdf_inverts_quantile(mean, sd, percent):
    dist = NormalDistribution(mean=mean, sd=sd)
    hist = numeric(dist, num_bins=200, bin_sizing="mass", warn=False)
    assert 100 * hist.cdf(hist.percentile(percent)) == approx(percent, abs=0.1)


@given(
    mean1=st.floats(min_value=100, max_value=100),
    mean2=st.floats(min_value=100, max_value=100),
    sd1=st.floats(min_value=0.01, max_value=100),
    sd2=st.floats(min_value=0.01, max_value=100),
    percent=st.integers(min_value=1, max_value=99),
)
@example(mean1=100, mean2=100, sd1=1, sd2=99, percent=2)
def test_quantile_mass_after_sum(mean1, mean2, sd1, sd2, percent):
    dist1 = NormalDistribution(mean=mean1, sd=sd1)
    dist2 = NormalDistribution(mean=mean2, sd=sd2)
    hist1 = numeric(dist1, num_bins=200, bin_sizing="mass", warn=False)
    hist2 = numeric(dist2, num_bins=200, bin_sizing="mass", warn=False)
    hist_sum = hist1 + hist2
    assert hist_sum.percentile(percent) == approx(
        stats.norm.ppf(percent / 100, mean1 + mean2, np.sqrt(sd1**2 + sd2**2)),
        rel=0.01 * (mean1 + mean2),
    )
    assert 100 * stats.norm.cdf(
        hist_sum.percentile(percent), hist_sum.exact_mean, hist_sum.exact_sd
    ) == approx(percent, abs=0.25)


@patch.object(np.random, "uniform", Mock(return_value=0.5))
@given(
    mean=st.floats(min_value=-10, max_value=10),
    bin_sizing=st.sampled_from(["uniform", "ev", "mass"]),
)
def test_sample(mean, bin_sizing):
    dist = NormalDistribution(mean=mean, sd=1)
    hist = numeric(dist, bin_sizing=bin_sizing)
    tol = 0.001 if bin_sizing == "uniform" else 0.01
    assert hist.sample() == approx(mean, rel=tol)


def test_utils_get_percentiles_basic():
    dist = NormalDistribution(mean=0, sd=1)
    hist = numeric(dist, warn=False)
    assert utils.get_percentiles(hist, 1) == hist.percentile(1)
    assert utils.get_percentiles(hist, [5]) == hist.percentile([5])
    assert all(utils.get_percentiles(hist, np.array([10, 20])) == hist.percentile([10, 20]))


def test_bump_indexes():
    assert _bump_indexes([2, 2, 2, 2], 4) == [0, 1, 2, 3]
    assert _bump_indexes([2, 2, 2, 2], 6) == [2, 3, 4, 5]
    assert _bump_indexes([2, 2, 2, 2], 8) == [2, 3, 4, 5]
    assert _bump_indexes([1, 2, 2, 5, 7, 7, 8, 8, 8], 9) == list(range(9))
    assert _bump_indexes([0, 0, 0, 6], 8) == [0, 1, 2, 6]
    assert _bump_indexes([0, 0, 0, 9, 9, 9], 11) == [0, 1, 2, 8, 9, 10]


def test_plot():
    return None
    hist = numeric(LognormalDistribution(norm_mean=0, norm_sd=1)) * numeric(
        NormalDistribution(mean=0, sd=5)
    )
    # hist = numeric(LognormalDistribution(norm_mean=0, norm_sd=2))
    hist.plot(scale="linear")


def test_performance():
    return None
    # Note: I wrote some C++ code to approximate the behavior of distribution
    # multiplication. On my machine, distribution multiplication (with profile
    # = False) runs in 15s, and the equivalent C++ code (with -O3) runs in 11s.
    # The C++ code is not well-optimized, the most glaring issue being it uses
    # std::sort instead of something like argpartition (the trouble is that
    # numpy's argpartition can partition on many values simultaneously, whereas
    # C++'s std::partition can only partition on one value at a time, which is
    # far slower).
    dist1 = LognormalDistribution(norm_mean=0, norm_sd=1)
    dist2 = LognormalDistribution(norm_mean=1, norm_sd=0.5)

    import cProfile
    import pstats
    import io

    pr = cProfile.Profile()
    pr.enable()

    for i in range(5000):
        hist1 = numeric(dist1, num_bins=100, bin_sizing="fat-hybrid")
        hist2 = numeric(dist2, num_bins=100, bin_sizing="fat-hybrid")
        hist1 = hist1 * hist2

    pr.disable()
    s = io.StringIO()
    sortby = "cumulative"
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
    ps.print_stats()
    print(s.getvalue())
